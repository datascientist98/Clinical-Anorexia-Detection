{"cells":[{"cell_type":"markdown","source":["Necessary imports"],"metadata":{"id":"jb3VcRn-1fG-"}},{"cell_type":"code","source":["#importing the necessary packages and libraries\n","import pandas as pd\n","import numpy as np\n","\n","import torch\n","from torchvision import datasets, transforms\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","from torchvision.utils import make_grid\n","\n","import os\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import xml.etree.cElementTree as et\n","from collections import defaultdict"],"metadata":{"id":"DUVq1sMi1dF4","executionInfo":{"status":"ok","timestamp":1651190017352,"user_tz":240,"elapsed":9209,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["Installing transformers and importing the tokenizer as well as the model of DistilBERT\n","\n","\n"],"metadata":{"id":"uNfwJAKs1cU-"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14592,"status":"ok","timestamp":1651190031939,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"},"user_tz":240},"id":"xwBQ-Vm13Kni","outputId":"27cc0d42-1fd0-4632-efb5-0cdc012e5eb9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 43.3 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 28.9 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 13.9 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 7.7 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n"]}],"source":["#installing transformers\n","!pip3 install transformers"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"QTY09Rc3-YRM","executionInfo":{"status":"ok","timestamp":1651190058626,"user_tz":240,"elapsed":654,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"}}},"outputs":[],"source":["from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n","from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification, DistilBertConfig, DistilBertTokenizerFast, TFDistilBertModel\n","from transformers import Trainer, TrainingArguments"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Eojc1e5r-a17","executionInfo":{"status":"ok","timestamp":1651190059183,"user_tz":240,"elapsed":3,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"}}},"outputs":[],"source":["model_name = \"distilbert-base-uncased\" #setting model name as the transformer we want to use, i.e. distilBERT"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Wu1uNz0RD86D","executionInfo":{"status":"ok","timestamp":1651190152178,"user_tz":240,"elapsed":181,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"}}},"outputs":[],"source":["max_length = 128 #setting a value for max_length which is the max number of words to tokenize in a given text"]},{"cell_type":"markdown","source":["Reading the training, validation and testing files"],"metadata":{"id":"HGz-ZtSr1w-Q"}},{"cell_type":"code","execution_count":37,"metadata":{"id":"zb0N-rTn-jV_","executionInfo":{"status":"ok","timestamp":1651191094246,"user_tz":240,"elapsed":2194,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"}}},"outputs":[],"source":["#reading the input files\n","training_df = pd.read_csv('training_df.csv')\n","validation_df = pd.read_csv('validation_df.csv')\n","testing_df = pd.read_csv('testing_df.csv')"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"OMr0Fk7b-jZf","executionInfo":{"status":"ok","timestamp":1651191094399,"user_tz":240,"elapsed":156,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"}}},"outputs":[],"source":["#replacing the null strings by empty strings. This is required to use a transformer\n","training_df = training_df.fillna('')\n","validation_df = validation_df.fillna('')\n","testing_df = testing_df.fillna('')"]},{"cell_type":"code","source":["# over- and under-sampling to help address class imbalance\n","positive_samples = training_df.loc[training_df[\"Ground_Truth\"] == 1]\n","negative_samples = training_df.loc[training_df[\"Ground_Truth\"] == 0]\n","training_df = pd.concat([positive_samples, positive_samples, negative_samples.sample(frac=0.2)], ignore_index=True)"],"metadata":{"id":"DsZ68Hp8J_oX","executionInfo":{"status":"ok","timestamp":1651191094399,"user_tz":240,"elapsed":2,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["len(training_df.loc[training_df[\"Ground_Truth\"]==1])/len(training_df.loc[training_df[\"Ground_Truth\"] == 0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fHXWzLVhmW6M","executionInfo":{"status":"ok","timestamp":1651191095195,"user_tz":240,"elapsed":2,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"}},"outputId":"377d1865-1af5-4e8f-ee2b-234e9c1901dd"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9629773211862764"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","execution_count":41,"metadata":{"id":"iJT3UrRS-c0_","executionInfo":{"status":"ok","timestamp":1651191098566,"user_tz":240,"elapsed":173,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"}}},"outputs":[],"source":["train_text = training_df['Comments'].values.tolist()\n","val_text = validation_df['Comments'].values.tolist()\n","test_text = testing_df['Comments'].values.tolist()"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"FxgIEY4qOFYm","executionInfo":{"status":"ok","timestamp":1651191098716,"user_tz":240,"elapsed":1,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"}}},"outputs":[],"source":["train_labels = torch.tensor(training_df['Ground_Truth'].values.tolist())\n","val_labels = torch.tensor(validation_df['Ground_Truth'].values.tolist())\n","test_labels = torch.tensor(testing_df['Ground_Truth'].values.tolist())"]},{"cell_type":"markdown","source":["Batch encoding"],"metadata":{"id":"Mr5NhDzd27I8"}},{"cell_type":"code","execution_count":43,"metadata":{"id":"4xWYufZaT3HM","executionInfo":{"status":"ok","timestamp":1651191099532,"user_tz":240,"elapsed":144,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"}}},"outputs":[],"source":["## writing a function to perform batch encoding of texts\n","## input: tokenizer object, list of text strings, batch size and max_length\n","# output: senquence of texts and corresponding attention masks encoded as tensor objects\n","def batch_encode(tokenizer, texts, batch_size=256, max_length=max_length):\n","    input_ids = []\n","    attention_mask = []\n","    \n","    for i in range(0, len(texts), batch_size):\n","        batch = texts[i:i+batch_size]\n","        inputs = tokenizer.batch_encode_plus(batch,\n","                                             max_length=max_length, \n","                                             padding='max_length', #dynamic padding \n","                                             truncation=True, \n","                                             return_attention_mask=True, #to return attention mask\n","                                             return_token_type_ids=False, #to return the token type IDS\n","                                             )\n","        input_ids.extend(inputs['input_ids'])\n","        attention_mask.extend(inputs['attention_mask'])\n","    \n","    \n","    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)"]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":1145,"status":"ok","timestamp":1651191101408,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"},"user_tz":240},"id":"Qj13_EuQT-eV"},"outputs":[],"source":["#initializing tokenizer\n","tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)"]},{"cell_type":"markdown","source":["Implementing the batch_encode function defined above to tokenize the train, validation and test text"],"metadata":{"id":"5k0v4hcd3E5Q"}},{"cell_type":"code","execution_count":45,"metadata":{"id":"aTdzvtIYT3vZ","executionInfo":{"status":"ok","timestamp":1651191109050,"user_tz":240,"elapsed":6643,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"}}},"outputs":[],"source":["X_train_ids, X_train_attention = batch_encode(tokenizer, train_text) # X_train_attention"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"miEUHdFqVYO6","executionInfo":{"status":"ok","timestamp":1651191112678,"user_tz":240,"elapsed":3631,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"}}},"outputs":[],"source":["X_val_ids, X_val_attention = batch_encode(tokenizer, val_text[:20000]) # X_val_attention"]},{"cell_type":"code","execution_count":78,"metadata":{"id":"c3aYw20JEYsn","executionInfo":{"status":"ok","timestamp":1651195240697,"user_tz":240,"elapsed":7764,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"}}},"outputs":[],"source":["X_test_ids, X_test_attention = batch_encode(tokenizer, test_text[:50000])"]},{"cell_type":"markdown","source":["Setting an intial configuration for the pretrained DistilBERT Model"],"metadata":{"id":"wZfSh3FW3Nka"}},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":479,"status":"ok","timestamp":1651191114975,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"},"user_tz":240},"id":"-209rGzWDsQy","outputId":"4e402ef0-58e8-4269-980e-5b34ce7d51fc"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'activation_13']\n","- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"]}],"source":["config = DistilBertConfig(dropout=0.2, #dropout for fully connected layers\n","                          attention_dropout=0.2, #dropout for attention probabilities\n","                          output_hidden_states=True) #returning the hidden states of all layers\n","distilBERT = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n","for layer in distilBERT.layers:\n","    layer.trainable = False #freezing the layers"]},{"cell_type":"markdown","source":["Building the model"],"metadata":{"id":"WAF4QHaC4YLl"}},{"cell_type":"code","execution_count":49,"metadata":{"id":"B5ZuC2DRAU1L","executionInfo":{"status":"ok","timestamp":1651191115116,"user_tz":240,"elapsed":3,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"}}},"outputs":[],"source":["max_length = 128\n","dropout = 0.2\n","lr = 0.001\n","random_state = 42 #ensuring reproducibility\n","\n","# defining a function to build a model of DistilBERT architecture\n","# input: a base Huggingface transformer model without a classification head, max_length\n","# output: a compiled keras model with custom classification layers built on top of the base architecture\n","\n","def build_model(transformer, max_length=max_length):\n","    \n","    weight_initializer = tf.keras.initializers.GlorotNormal(seed=random_state) \n","    \n","    input_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n","                                            name='input_ids', \n","                                            dtype='int32')\n","    input_attention_layer = tf.keras.layers.Input(shape=(max_length,), \n","                                                  name='input_attention', \n","                                                  dtype='int32')\n","\n","    last_hidden_state = transformer([input_ids_layer, input_attention_layer])[0] #the element at index 0 is the hidden state at the output of the model's last layer\n","    \n","    cls_token = last_hidden_state[:, 0, :]\n","\n","    x = tf.keras.layers.Dense(8, activation=tf.nn.relu)(cls_token)\n","    x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(x)\n","    \n","    output = tf.keras.layers.Dense(1, \n","                                   activation='sigmoid',\n","                                   kernel_initializer=weight_initializer,  \n","                                   kernel_constraint=None,\n","                                   bias_initializer='zeros'\n","                                   )(x) #single node for binary classification\n","\n","    model = tf.keras.Model([input_ids_layer, input_attention_layer], output) #model definiton\n","\n","    model.compile(tf.keras.optimizers.Adam(learning_rate=lr), #model compiling using Adam optimizer, binary cross entropy loss and binary accuracy metric\n","                  loss=tf.keras.losses.BinaryFocalCrossentropy(),\n","                  metrics=[tf.keras.metrics.BinaryAccuracy()])\n","    \n","    return model"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"EIGtf_Z3Eiu9","executionInfo":{"status":"ok","timestamp":1651191116102,"user_tz":240,"elapsed":988,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"}}},"outputs":[],"source":["model = build_model(distilBERT, max_length=max_length) #building model"]},{"cell_type":"markdown","source":["Training the model"],"metadata":{"id":"aWpCRLur5YKG"}},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2187750,"status":"ok","timestamp":1651193303846,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"},"user_tz":240},"id":"AXYRFPDBOgHm","outputId":"197d8b11-f249-4e84-8536-828b7aea804a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","474/474 - 220s - loss: 0.1412 - binary_accuracy: 0.7143 - val_loss: 0.1611 - val_binary_accuracy: 0.7094 - 220s/epoch - 464ms/step\n","Epoch 2/10\n","474/474 - 212s - loss: 0.1334 - binary_accuracy: 0.7365 - val_loss: 0.1389 - val_binary_accuracy: 0.7520 - 212s/epoch - 448ms/step\n","Epoch 3/10\n","474/474 - 213s - loss: 0.1318 - binary_accuracy: 0.7403 - val_loss: 0.1211 - val_binary_accuracy: 0.8099 - 213s/epoch - 448ms/step\n","Epoch 4/10\n","474/474 - 212s - loss: 0.1304 - binary_accuracy: 0.7460 - val_loss: 0.1428 - val_binary_accuracy: 0.7384 - 212s/epoch - 448ms/step\n","Epoch 5/10\n","474/474 - 212s - loss: 0.1295 - binary_accuracy: 0.7468 - val_loss: 0.1532 - val_binary_accuracy: 0.7132 - 212s/epoch - 448ms/step\n","Epoch 6/10\n","474/474 - 212s - loss: 0.1286 - binary_accuracy: 0.7502 - val_loss: 0.1341 - val_binary_accuracy: 0.7656 - 212s/epoch - 448ms/step\n","Epoch 7/10\n","474/474 - 213s - loss: 0.1274 - binary_accuracy: 0.7536 - val_loss: 0.1379 - val_binary_accuracy: 0.7549 - 213s/epoch - 449ms/step\n","Epoch 8/10\n","474/474 - 212s - loss: 0.1272 - binary_accuracy: 0.7527 - val_loss: 0.1480 - val_binary_accuracy: 0.7232 - 212s/epoch - 448ms/step\n","Epoch 9/10\n","474/474 - 213s - loss: 0.1272 - binary_accuracy: 0.7555 - val_loss: 0.1326 - val_binary_accuracy: 0.7804 - 213s/epoch - 449ms/step\n","Epoch 10/10\n","474/474 - 212s - loss: 0.1260 - binary_accuracy: 0.7560 - val_loss: 0.1381 - val_binary_accuracy: 0.7603 - 212s/epoch - 448ms/step\n"]}],"source":["epochs = 10\n","batch_size = 64\n","num_steps = len(training_df['Comments'].index) // batch_size\n","\n","# Train the model\n","training = model.fit(\n","    x = [X_train_ids, X_train_attention],\n","    y = train_labels.numpy(),\n","    epochs = epochs,\n","    batch_size = batch_size,\n","    steps_per_epoch = num_steps,\n","    validation_data = ([X_val_ids, X_val_attention], val_labels[:20000].numpy()),\n","    verbose=2\n",")"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"afK3lbiq9jUE","executionInfo":{"status":"ok","timestamp":1651194118661,"user_tz":240,"elapsed":1166,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"}}},"outputs":[],"source":["model.save_weights(\"transformer_ci3_weights.h5\") #saving the weights of the model"]},{"cell_type":"code","source":["model.load_weights(\"transformer_ci3_weights.h5\")"],"metadata":{"id":"vTnnlhC7I9k6","executionInfo":{"status":"ok","timestamp":1651194121826,"user_tz":240,"elapsed":394,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":82232,"status":"ok","timestamp":1651194205997,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"},"user_tz":240},"id":"p9H9DnoMcJeg","outputId":"251f74ef-a98b-43ed-8b0b-4a39b686069e"},"outputs":[{"output_type":"stream","name":"stdout","text":["157/157 [==============================] - 42s 267ms/step - loss: 0.1242 - binary_accuracy: 0.7956\n"]}],"source":["results = model.evaluate(x = [X_test_ids, X_test_attention],y = np.array(test_labels[:10000]),batch_size = batch_size)"]},{"cell_type":"code","execution_count":95,"metadata":{"id":"Uk8Dc9acdwMC","executionInfo":{"status":"ok","timestamp":1651195880745,"user_tz":240,"elapsed":142103,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"}}},"outputs":[],"source":["predictions= model.predict([X_val_ids[:20000], X_val_attention[:20000]])"]},{"cell_type":"code","execution_count":96,"metadata":{"id":"xoWGhBAi7wjL","executionInfo":{"status":"ok","timestamp":1651195880745,"user_tz":240,"elapsed":5,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"}}},"outputs":[],"source":["prediction_labels = np.argmax(predictions, axis=1)"]},{"cell_type":"code","source":["set(prediction_labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-9D4CKGmb5Qo","executionInfo":{"status":"ok","timestamp":1651195880746,"user_tz":240,"elapsed":6,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"}},"outputId":"ea8ae27b-e121-41ee-9793-51ccb2e66dfd"},"execution_count":97,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0}"]},"metadata":{},"execution_count":97}]},{"cell_type":"code","execution_count":98,"metadata":{"id":"G5NPq06P-UsJ","executionInfo":{"status":"ok","timestamp":1651195880746,"user_tz":240,"elapsed":5,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"}}},"outputs":[],"source":["true_labels = np.array(val_labels[:20000])"]},{"cell_type":"code","execution_count":99,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1651195880746,"user":{"displayName":"Sara Nayak","userId":"16216081765611070474"},"user_tz":240},"id":"i7xn6Ha-d649","outputId":"5bd95708-81e1-4cc4-fcef-e1bfb9ceeba7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy-Score of Model is:  0.8856\n","Balanced-Accuracy-Score of Model is:  0.5\n","Recall-Score of Model is:  0.0\n","Precision-Score of Model is:  1.0\n","F1-Score of Model is:  0.0\n"]}],"source":["from sklearn.metrics import accuracy_score, recall_score, precision_score, balanced_accuracy_score, f1_score\n","print(\"Accuracy-Score of Model is: \", accuracy_score(true_labels,prediction_labels))\n","print(\"Balanced-Accuracy-Score of Model is: \", balanced_accuracy_score(true_labels,prediction_labels))\n","print(\"Recall-Score of Model is: \", recall_score(true_labels,prediction_labels,zero_division=1))\n","print(\"Precision-Score of Model is: \", precision_score(true_labels,prediction_labels,zero_division=1))\n","print(\"F1-Score of Model is: \", f1_score(true_labels,prediction_labels))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"CIS 522 Transformers Model.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
